 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f801558-6815-4e67-9db2-486adaf3606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praveen kumar\\AppData\\Local\\Temp\\ipykernel_19352\\3268164514.py:21: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [        # ================================
# Car Price Prediction - Fully Runnable Example
# ================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# ================================
# Step 1: Create Synthetic Dataset
# ================================

np.random.seed(42)

n_samples = 500

brands = ['Toyota', 'Honda', 'Ford', 'BMW', 'Audi']
fuel_types = ['Petrol', 'Diesel', 'CNG', 'Electric']
transmissions = ['Manual', 'Automatic']

df = pd.DataFrame({
    'brand': np.random.choice(brands, n_samples),
    'year': np.random.randint(2005, 2023, n_samples),
    'mileage': np.random.randint(5000, 200000, n_samples),
    'engine': np.random.randint(1000, 4000, n_samples),  # in cc
    'fuel_type': np.random.choice(fuel_types, n_samples),
    'transmission': np.random.choice(transmissions, n_samples)
})

# Simulate price based on features
base_price = 5000
df['price'] = (
    base_price 
    + (df['year'] - 2000) * 500 
    - df['mileage'] * 0.05 
    + (df['engine'] * 2)
    + np.random.normal(0, 1000, n_samples)  # random noise
)

print("Synthetic dataset created. First 5 rows:")
display(df.head())

# ================================
# Step 2: Preprocessing
# ================================

X = df.drop('price', axis=1)
y = df['price']

categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ]
)

# ================================
# Step 3: Train-Test Split
# ================================

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ================================
# Step 4: Train Models
# ================================

# Linear Regression
lr_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])
lr_pipeline.fit(X_train, y_train)
y_pred_lr = lr_pipeline.predict(X_test)

# Random Forest
rf_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
])
rf_pipeline.fit(X_train, y_train)
y_pred_rf = rf_pipeline.predict(X_test)

# ================================
# Step 5: Evaluate Models
# ================================

def evaluate_model(name, y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    print(f"{name} — RMSE: {rmse:.2f}, R²: {r2:.2f}")

print("\nModel Performance:")
evaluate_model("Linear Regression", y_test, y_pred_lr)
evaluate_model("Random Forest", y_test, y_pred_rf)

# ================================
# Step 6: Feature Importance (Random Forest)
# ================================

rf_model = rf_pipeline.named_steps['regressor']

# Get feature names after one-hot encoding
encoded_features = list(
    rf_pipeline.named_steps['preprocessor']
    .transformers_[1][1]
    .get_feature_names_out(categorical_cols)
)
all_features = numerical_cols + encoded_features

importances = rf_model.feature_importances_
feat_imp = pd.Series(importances, index=all_features).sort_values(ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x=feat_imp[:10], y=feat_imp.index[:10])
plt.title("Top 10 Feature Importances (Random Forest)")
plt.show()]

